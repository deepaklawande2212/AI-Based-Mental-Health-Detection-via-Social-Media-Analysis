"""
CSV Data Processing API Routes

This module provides REST API endpoints for CSV file upload and analysis.

Endpoints:
- POST /upload - Upload and process CSV file
- GET /data/{file_id} - Get processed CSV data
- POST /analyze/{file_id} - Analyze CSV data
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Body, Request
from typing import Optional, List, Any
from datetime import datetime
from loguru import logger
from pydantic import BaseModel, Field
import csv
import io
import uuid
import base64

from app.models.request_models import CSVUploadResponse, AnalysisResponse
from app.models.database_models import CSVData, AnalysisResult
from app.services.csv_service import CSVService
from app.services.ai_models import ModelManager
from app.config.database import get_csv_collection, get_analysis_collection
from app.services.emotion_detection import EmotionDetector
from app.services.recommendations import generate_analysis_recommendations


def _calculate_realistic_confidence(model_results: List[Any]) -> float:
    """Calculate a realistic overall confidence score based on model results"""
    if not model_results:
        return 0.5
    
    # Filter successful models and their confidences
    successful_confidences = []
    for result in model_results:
        if result.status == "success":
            # Cap confidence at 0.95 to avoid unrealistic 100% scores
            confidence = min(result.confidence, 0.95)
            successful_confidences.append(confidence)
    
    if not successful_confidences:
        return 0.5
    
    # Calculate simple average confidence (more reliable than weighted average)
    average_confidence = sum(successful_confidences) / len(successful_confidences)
    
    # Ensure confidence is between 0.5 and 0.95
    realistic_confidence = max(0.5, min(0.95, average_confidence))
    
    return realistic_confidence


# Request model for direct CSV analysis
class CSVAnalysisRequest(BaseModel):
    file_content: str = Field(..., description="Base64 encoded CSV content or plain text")
    filename: str = Field(..., description="Original filename")
    analysis_type: Optional[str] = Field(default="comprehensive", description="Type of analysis")
    text_column: Optional[str] = Field(default="text", description="Column containing text data")


# Request model for simple text analysis
class SimpleTextAnalysisRequest(BaseModel):
    text: str = Field(..., description="Plain text to analyze")
    analysis_type: Optional[str] = Field(default="comprehensive", description="Type of analysis")


router = APIRouter()


@router.post("/upload", response_model=CSVUploadResponse)
async def upload_csv_file(
    file: UploadFile = File(...),
    text_column: str = Form("text"),
    description: Optional[str] = Form(None)
):
    """
    Upload and process a CSV file for text analysis
    
    Args:
        file: Uploaded CSV file
        text_column: Name of the column containing text data
        description: Optional description of the dataset
        
    Returns:
        CSVUploadResponse: Processing status and file info
    """
    try:
        logger.info(f"üìÅ API request to upload CSV file: {file.filename}")
        
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")
        
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="File must be a CSV file")
        
        # Read file content
        content = await file.read()
        
        # Validate file size (max 10MB)
        if len(content) > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large (max 10MB)")
        
        # Process CSV content
        csv_service = CSVService()
        
        try:
            # Parse CSV
            content_str = content.decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content_str))
            
            # Check if text column exists
            if csv_reader.fieldnames is None:
                raise HTTPException(status_code=400, detail="Invalid CSV format")
            
            if text_column not in csv_reader.fieldnames:
                available_columns = ", ".join(csv_reader.fieldnames)
                raise HTTPException(
                    status_code=400,
                    detail=f"Column '{text_column}' not found. Available columns: {available_columns}"
                )
            
            # Process the data
            processed_data = await csv_service.process_csv_data(
                content_str,
                text_column,
                file.filename,
                description
            )
            
            return CSVUploadResponse(
                file_id=processed_data.file_id,
                filename=processed_data.filename,
                file_size=processed_data.file_size,
                text_column=processed_data.text_column,
                total_rows=processed_data.total_rows or processed_data.row_count,
                valid_rows=processed_data.valid_text_rows,
                row_count=processed_data.row_count,
                columns=processed_data.headers,
                processing_status=processed_data.processing_status,
                status=processed_data.processing_status,
                description=processed_data.description,
                created_at=processed_data.created_at or processed_data.upload_date,
                upload_date=processed_data.upload_date
            )
            
        except UnicodeDecodeError:
            raise HTTPException(status_code=400, detail="File encoding not supported. Please use UTF-8.")
        except csv.Error as e:
            raise HTTPException(status_code=400, detail=f"CSV parsing error: {str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error uploading CSV file: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/data/{file_id}", response_model=CSVUploadResponse)
async def get_csv_data(file_id: str):
    """
    Get processed CSV data by file ID
    
    Args:
        file_id: Unique file identifier
        
    Returns:
        CSVUploadResponse: CSV data information
    """
    try:
        logger.info(f"üìä API request to get CSV data for file: {file_id}")
        
        collection = await get_csv_collection()
        doc = await collection.find_one({"file_id": file_id})
        
        if not doc:
            raise HTTPException(
                status_code=404,
                detail=f"CSV file with ID {file_id} not found"
            )
        
        csv_data = CSVData(**doc)
        
        return CSVUploadResponse(
            file_id=csv_data.file_id,
            filename=csv_data.filename,
            file_size=csv_data.file_size,
            text_column=csv_data.text_column,
            total_rows=csv_data.total_rows or csv_data.row_count,
            valid_rows=csv_data.valid_text_rows,
            row_count=csv_data.row_count,
            columns=csv_data.headers,
            processing_status=csv_data.processing_status,
            status=csv_data.processing_status,
            description=csv_data.description,
            created_at=csv_data.created_at or csv_data.upload_date,
            upload_date=csv_data.upload_date
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error getting CSV data: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.post("/analyze/{file_id}", response_model=AnalysisResponse)
async def analyze_csv_data(file_id: str):
    """
    Analyze CSV data using AI models
    
    Args:
        file_id: CSV file identifier
        
    Returns:
        AnalysisResponse: Analysis results from all models
    """
    try:
        logger.info(f"üîÆ API request to analyze CSV data for file: {file_id}")
        
        # Get CSV data
        collection = await get_csv_collection()
        doc = await collection.find_one({"file_id": file_id})
        
        if not doc:
            raise HTTPException(
                status_code=404,
                detail=f"CSV file with ID {file_id} not found"
            )
        
        csv_data = CSVData(**doc)
        
        if csv_data.processing_status != "completed":
            raise HTTPException(
                status_code=400,
                detail="CSV processing is not completed yet"
            )
        
        if not csv_data.processed_texts or len(csv_data.processed_texts) == 0:
            raise HTTPException(
                status_code=400,
                detail="No text data available for analysis"
            )
        
        # Combine text data for analysis
        combined_text = " ".join(csv_data.processed_texts)
        
        # Limit text size for processing (max 100k characters)
        if len(combined_text) > 100000:
            combined_text = combined_text[:100000]
            logger.warning(f"‚ö†Ô∏è Text truncated for analysis (original: {len(' '.join(csv_data.processed_texts))} chars)")
        
        # Get model manager from app state
        from app.main import app
        if not hasattr(app.state, 'model_manager'):
            raise HTTPException(
                status_code=503,
                detail="AI models not available"
            )
        
        model_manager = app.state.model_manager
        
        # Run AI analysis
        model_predictions = await model_manager.predict_all_models(combined_text)
        
        if not model_predictions:
            raise HTTPException(
                status_code=500,
                detail="No model predictions were successful"
            )
        
        # Calculate aggregated results (use best performing model)
        best_model = max(model_predictions, key=lambda x: x.accuracy if x.status == "success" else 0)
        
        # Create analysis result with improved final risk calculation
        analysis_id = str(uuid.uuid4())
        
        # Calculate final risk level - use the HIGHEST risk detected by any model
        final_risk_level = 'low'
        final_risk_score = 0.2
        
        for pred in model_predictions:
            if pred.status == "success" and pred.risk:
                if pred.risk.level == 'high':
                    final_risk_level = 'high'
                    final_risk_score = max(final_risk_score, 0.8)
                elif pred.risk.level == 'medium' and final_risk_level != 'high':
                    final_risk_level = 'medium'
                    final_risk_score = max(final_risk_score, 0.5)
        
        # CRITICAL: Override final risk for self-harm content
        text_lower = combined_text.lower()
        suicide_self_harm_keywords = [
            'hurt myself', 'cut myself', 'kill myself', 'want to die', 'end it all', 
            'suicidal', 'suicide', 'self-harm', 'self harm', 'hopeless', 'worthless'
        ]
        
        if any(keyword in text_lower for keyword in suicide_self_harm_keywords):
            final_risk_level = 'high'
            final_risk_score = 0.9
            logger.warning(f"üö® CRITICAL: Self-harm content detected - Final risk set to HIGH")
        
        # Create final risk assessment
        from app.models.request_models import RiskResponse
        final_risk = RiskResponse(
            level=final_risk_level,
            score=final_risk_score,
            factors=['text analysis', 'model consensus'],
            recommendations=['Continue monitoring', 'Seek professional help' if final_risk_level == 'high' else 'Monitor closely'],
            confidence=0.85
        )
        
        # Calculate final sentiment - ensure correlation with risk level
        final_sentiment = best_model.sentiment if best_model.sentiment else None
        
        # CRITICAL: Override final sentiment for self-harm content
        if final_risk_level == 'high' and any(keyword in text_lower for keyword in suicide_self_harm_keywords):
            # Force negative sentiment for high-risk self-harm content
            from app.models.request_models import SentimentResponse
            final_sentiment = SentimentResponse(
                positive=0.05,
                negative=0.85,
                neutral=0.10,
                overall="negative",
                confidence=0.95
            )
            logger.warning(f"üö® CRITICAL: Self-harm content detected - Final sentiment set to NEGATIVE")
        
        analysis_result = AnalysisResult(
            analysis_id=analysis_id,
            analysis_type="csv",
            file_id=file_id,
            model_predictions=model_predictions,
            final_sentiment=final_sentiment,
            final_emotions=best_model.emotions if best_model.emotions else None,
            final_risk=final_risk,
            best_performing_model=best_model.model_name,
            successful_models=[p.model_name for p in model_predictions if p.status == "success"],
            failed_models=[p.model_name for p in model_predictions if p.status == "failed"],
            processing_time=sum(p.prediction_time for p in model_predictions),
            status="completed"
        )
        
        # Store analysis result
        analysis_collection = await get_analysis_collection()
        await analysis_collection.insert_one(analysis_result.dict(exclude={"id"}))
        
        # Convert model predictions to response format
        from app.models.request_models import ModelResultResponse, SentimentResponse, EmotionResponse, RiskResponse
        
        model_results = []
        for pred in model_predictions:
            sentiment_resp = None
            if pred.sentiment:
                sentiment_resp = SentimentResponse(
                    positive=pred.sentiment.positive,
                    negative=pred.sentiment.negative,
                    neutral=pred.sentiment.neutral,
                    overall=pred.sentiment.overall,
                    confidence=pred.sentiment.confidence
                )
            
            emotion_resp = None
            if pred.emotions:
                emotion_resp = EmotionResponse(
                    joy=pred.emotions.joy,
                    sadness=pred.emotions.sadness,
                    anger=pred.emotions.anger,
                    fear=pred.emotions.fear,
                    surprise=pred.emotions.surprise,
                    disgust=pred.emotions.disgust,
                    dominant_emotion=pred.emotions.dominant_emotion,
                    confidence=pred.emotions.confidence
                )
            
            risk_resp = None
            if pred.risk:
                risk_resp = RiskResponse(
                    level=pred.risk.level,
                    score=pred.risk.score,
                    factors=pred.risk.factors,
                    recommendations=pred.risk.recommendations,
                    confidence=pred.risk.confidence
                )
            
            model_results.append(ModelResultResponse(
                model_name=pred.model_name,
                accuracy=pred.accuracy,
                processing_time=pred.prediction_time,
                status=pred.status,
                confidence=pred.confidence,
                sentiment=sentiment_resp,
                emotions=emotion_resp,
                risk=risk_resp,
                error_message=pred.error_message
            ))
        
        # Create final responses
        final_sentiment = None
        if analysis_result.final_sentiment:
            final_sentiment = SentimentResponse(
                positive=analysis_result.final_sentiment.positive,
                negative=analysis_result.final_sentiment.negative,
                neutral=analysis_result.final_sentiment.neutral,
                overall=analysis_result.final_sentiment.overall,
                confidence=analysis_result.final_sentiment.confidence
            )
        
        final_emotions = None
        if analysis_result.final_emotions:
            final_emotions = EmotionResponse(
                joy=analysis_result.final_emotions.joy,
                sadness=analysis_result.final_emotions.sadness,
                anger=analysis_result.final_emotions.anger,
                fear=analysis_result.final_emotions.fear,
                surprise=analysis_result.final_emotions.surprise,
                disgust=analysis_result.final_emotions.disgust,
                dominant_emotion=analysis_result.final_emotions.dominant_emotion,
                confidence=analysis_result.final_emotions.confidence
            )
        
        final_risk = None
        if analysis_result.final_risk:
            final_risk = RiskResponse(
                level=analysis_result.final_risk.level,
                score=analysis_result.final_risk.score,
                factors=analysis_result.final_risk.factors,
                recommendations=analysis_result.final_risk.recommendations,
                confidence=analysis_result.final_risk.confidence
            )
        
        return AnalysisResponse(
            analysis_id=analysis_id,
            analysis_type="csv",
            status="completed",
            model_results=model_results,
            final_sentiment=final_sentiment,
            final_emotions=final_emotions,
            final_risk=final_risk,
            best_model=analysis_result.best_performing_model,
            successful_models=analysis_result.successful_models,
            failed_models=analysis_result.failed_models,
            processing_time=analysis_result.processing_time,
            created_at=analysis_result.created_at,
            confidence=best_model.confidence if best_model else 0.0
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error analyzing CSV data: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/files", response_model=List[CSVUploadResponse])
async def list_csv_files(limit: int = 50, skip: int = 0):
    """
    List all uploaded CSV files
    
    Args:
        limit: Maximum number of files to return
        skip: Number of files to skip
        
    Returns:
        List[CSVUploadResponse]: List of CSV files
    """
    try:
        collection = await get_csv_collection()
        
        cursor = collection.find({}).sort("upload_date", -1).skip(skip).limit(limit)
        docs = await cursor.to_list(length=limit)
        
        files = []
        for doc in docs:
            csv_data = CSVData(**doc)
            files.append(CSVUploadResponse(
                file_id=csv_data.file_id,
                filename=csv_data.filename,
                file_size=csv_data.file_size,
                total_rows=csv_data.total_rows or csv_data.row_count,
                valid_rows=csv_data.valid_text_rows,
                row_count=csv_data.row_count,
                columns=csv_data.headers,
                text_column=csv_data.text_column,
                processing_status=csv_data.processing_status,
                status=csv_data.processing_status,
                description=csv_data.description,
                created_at=csv_data.created_at or csv_data.upload_date,
                upload_date=csv_data.upload_date
            ))
        
        return files
        
    except Exception as e:
        logger.error(f"‚ùå Error listing CSV files: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.delete("/data/{file_id}")
async def delete_csv_file(file_id: str):
    """
    Delete a CSV file and its data
    
    Args:
        file_id: File identifier
        
    Returns:
        dict: Success message
    """
    try:
        collection = await get_csv_collection()
        result = await collection.delete_one({"file_id": file_id})
        
        if result.deleted_count == 0:
            raise HTTPException(
                status_code=404,
                detail=f"CSV file with ID {file_id} not found"
            )
        
        logger.info(f"üóëÔ∏è Deleted CSV file: {file_id}")
        return {"message": f"CSV file {file_id} deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error deleting CSV file: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/sample/{file_id}")
async def get_csv_sample(file_id: str, limit: int = 10):
    """
    Get a sample of text data from CSV file
    
    Args:
        file_id: File identifier
        limit: Number of sample texts to return
        
    Returns:
        dict: Sample text data
    """
    try:
        collection = await get_csv_collection()
        doc = await collection.find_one({"file_id": file_id})
        
        if not doc:
            raise HTTPException(
                status_code=404,
                detail=f"CSV file with ID {file_id} not found"
            )
        
        csv_data = CSVData(**doc)
        
        if not csv_data.processed_texts:
            return {"sample_texts": [], "total_available": 0}
        
        sample_size = min(limit, len(csv_data.processed_texts))
        sample_texts = csv_data.processed_texts[:sample_size]
        
        return {
            "sample_texts": sample_texts,
            "sample_size": sample_size,
            "total_available": len(csv_data.processed_texts),
            "file_id": file_id,
            "filename": csv_data.filename
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error getting CSV sample: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_csv_direct(request: CSVAnalysisRequest, req: Request):
    """
    Analyze CSV data directly without separate upload step
    
    Args:
        request: CSV analysis request with file content and parameters
        req: FastAPI request object to access app state
        
    Returns:
        AnalysisResponse: Analysis results from all models
    """
    try:
        logger.info(f"üîÆ Direct CSV analysis request for: {request.filename}")
        
        # Decode file content if it's base64 encoded
        try:
            csv_content = base64.b64decode(request.file_content).decode('utf-8')
        except:
            # If not base64, assume it's plain text
            csv_content = request.file_content
        
        # Process CSV data using the service
        csv_service = CSVService()
        csv_data = await csv_service.process_csv_data(
            csv_content,
            request.text_column,
            request.filename,
            f"Direct analysis ({request.analysis_type})"
        )
        
        if not csv_data.processed_texts or len(csv_data.processed_texts) == 0:
            raise HTTPException(
                status_code=400,
                detail="No text data found for analysis"
            )
        
        # Combine text data for analysis
        combined_text = " ".join(csv_data.processed_texts)
        
        # Limit text size for processing (max 100k characters)
        if len(combined_text) > 100000:
            combined_text = combined_text[:100000]
            logger.warning(f"‚ö†Ô∏è Text truncated for analysis (original: {len(' '.join(csv_data.processed_texts))} chars)")
        
        # Get model manager from app state
        if not hasattr(req.app.state, 'model_manager'):
            raise HTTPException(
                status_code=503,
                detail="AI models not available"
            )
        
        model_manager = req.app.state.model_manager
        
        # Run AI analysis using the correct method
        model_predictions = await model_manager.predict_all(combined_text)
        
        if not model_predictions:
            raise HTTPException(
                status_code=500,
                detail="No model predictions were successful"
            )
        
        # SMART BEST MODEL SELECTION - Prioritize based on content type and risk level
        def select_best_model(predictions, text):
            """Select the most appropriate model based on content type and risk level"""
            text_lower = text.lower()
            
            # Define high-risk keywords
            high_risk_keywords = [
                'hurt myself', 'cut myself', 'kill myself', 'want to die', 'end it all', 
                'suicidal', 'suicide', 'self-harm', 'self harm', 'hopeless', 'worthless',
                'no point living', 'better off dead', 'give up', 'desperate'
            ]
            
            # Check if text contains high-risk content
            has_high_risk = any(keyword in text_lower for keyword in high_risk_keywords)
            
            # Priority order for different scenarios
            if has_high_risk:
                # For high-risk content, prioritize models that correctly identify negative sentiment
                priority_models = ['bert', 'lstm', 'cnn', 'rnn', 'decision_tree']
                for model_name in priority_models:
                    for pred in predictions:
                        if (pred.model_name == model_name and 
                            pred.status == "success" and 
                            pred.sentiment and 
                            pred.sentiment.negative > pred.sentiment.positive):
                            logger.info(f"üéØ Selected {model_name} for high-risk content (negative sentiment: {pred.sentiment.negative:.2f})")
                            return pred
            
            # For positive content, prioritize models that correctly identify positive sentiment
            positive_keywords = ['happy', 'wonderful', 'amazing', 'excellent', 'fantastic', 'love', 'joy', 'great', 'best', 'perfect']
            has_positive = any(keyword in text_lower for keyword in positive_keywords)
            
            if has_positive:
                priority_models = ['bert', 'decision_tree', 'cnn', 'lstm', 'rnn']
                for model_name in priority_models:
                    for pred in predictions:
                        if (pred.model_name == model_name and 
                            pred.status == "success" and 
                            pred.sentiment and 
                            pred.sentiment.positive > pred.sentiment.negative):
                            logger.info(f"üéØ Selected {model_name} for positive content (positive sentiment: {pred.sentiment.positive:.2f})")
                            return pred
            
            # For neutral content or fallback, use BERT (most reliable)
            for pred in predictions:
                if pred.model_name == 'bert' and pred.status == "success":
                    logger.info(f"üéØ Selected BERT as fallback (neutral content)")
                    return pred
            
            # Final fallback: highest confidence model
            best_model = max(predictions, key=lambda x: x.confidence if x.status == "success" else 0)
            logger.info(f"üéØ Selected {best_model.model_name} as final fallback (confidence: {best_model.confidence:.2f})")
            return best_model
        
        # Select best model using smart logic
        best_model = select_best_model(model_predictions, combined_text)
        
        # Create simplified analysis result for now
        analysis_id = str(uuid.uuid4())
        
        # Convert predictions to response format
        from app.models.request_models import ModelResultResponse, SentimentResponse, EmotionResponse, RiskResponse
        
        model_results = []
        successful_models = []
        failed_models = []
        
        for prediction in model_predictions:
            if prediction.status == "success":
                successful_models.append(prediction.model_name)
                
                # Create sentiment response
                if prediction.sentiment:
                    sentiment_resp = SentimentResponse(
                        positive=prediction.sentiment.positive,
                        negative=prediction.sentiment.negative,
                        neutral=prediction.sentiment.neutral,
                        overall=prediction.sentiment.overall,
                        confidence=prediction.sentiment.confidence
                    )
                else:
                    sentiment_resp = SentimentResponse(
                        positive=0.3,
                        negative=0.2,
                        neutral=0.5,
                        overall="neutral",
                        confidence=0.75
                    )
                
                # Create emotion response using real emotion detection
                if prediction.emotions:
                    # Convert EmotionScore to EmotionResponse
                    emotion_resp = EmotionResponse(
                        joy=prediction.emotions.joy,
                        sadness=prediction.emotions.sadness,
                        anger=prediction.emotions.anger,
                        fear=prediction.emotions.fear,
                        surprise=prediction.emotions.surprise,
                        disgust=prediction.emotions.disgust,
                        dominant_emotion=prediction.emotions.dominant_emotion,
                        confidence=prediction.emotions.confidence
                    )
                else:
                    # Use real emotion detection for models that don't have emotion analysis
                    emotion_detector = EmotionDetector()
                    emotion_analysis = await emotion_detector.detect_emotions(csv_content.strip())
                    emotion_resp = EmotionResponse(
                        joy=emotion_analysis['joy'],
                        sadness=emotion_analysis['sadness'],
                        anger=emotion_analysis['anger'],
                        fear=emotion_analysis['fear'],
                        surprise=emotion_analysis['surprise'],
                        disgust=emotion_analysis['disgust'],
                        dominant_emotion=emotion_analysis['dominant_emotion'],
                        confidence=emotion_analysis['confidence']
                    )
                
                # Create risk response
                if prediction.risk:
                    risk_resp = RiskResponse(
                        level=prediction.risk.level,
                        score=prediction.risk.score,
                        factors=prediction.risk.factors,
                        recommendations=prediction.risk.recommendations,
                        confidence=prediction.risk.confidence
                    )
                else:
                    # Generate intelligent recommendations for this model
                    model_analysis_data = {
                        'risk_level': 'low',
                        'emotions': emotion_resp.dict() if emotion_resp else emotion_analysis,
                        'sentiment': sentiment_resp.dict() if sentiment_resp else {'overall': 'neutral'},
                        'text_content': csv_content
                    }
                    
                    model_recommendations = generate_analysis_recommendations(model_analysis_data)
                    
                    risk_resp = RiskResponse(
                        level="low",
                        score=0.2,
                        factors=["text analysis"],
                        recommendations=model_recommendations,
                        confidence=0.75
                    )
                
                model_results.append(ModelResultResponse(
                    model_name=prediction.model_name,
                    accuracy=prediction.accuracy,
                    processing_time=prediction.prediction_time,
                    status="success",
                    confidence=prediction.confidence,
                    sentiment=sentiment_resp,
                    emotions=emotion_resp,
                    risk=risk_resp,
                    error_message=None
                ))
            else:
                failed_models.append(prediction.model_name)
                model_results.append(ModelResultResponse(
                    model_name=prediction.model_name,
                    accuracy=prediction.accuracy,
                    processing_time=prediction.prediction_time,
                    status="failed",
                    confidence=prediction.confidence,
                    sentiment=None,
                    emotions=None,
                    risk=None,
                    error_message=prediction.error_message
                ))
        
        # Determine best performing model (avoid artificially high confidence)
        best_performing_model = None
        if successful_models:
            # Find the model with highest realistic confidence (not 1.0)
            best_confidence = 0
            for result in model_results:
                if result.status == "success" and result.confidence < 1.0 and result.confidence > best_confidence:
                    best_confidence = result.confidence
                    best_performing_model = result.model_name
            
            # If no realistic confidence found, use the first successful model
            if not best_performing_model:
                best_performing_model = successful_models[0]
        
        # Create final responses using real emotion detection
        # Get the text content for emotion analysis
        text_content = ""
        if csv_data.processed_texts:
            text_content = " ".join(csv_data.processed_texts[:3])  # Use first 3 texts for analysis
        elif csv_data.data:
            # Extract text from CSV data
            text_columns = [col for col in csv_data.headers if 'text' in col.lower() or 'content' in col.lower()]
            if text_columns:
                for row in csv_data.data[:3]:  # Use first 3 rows
                    if text_columns[0] in row:
                        text_content += " " + str(row[text_columns[0]])
        
        # Analyze emotions using real emotion detection
        emotion_detector = EmotionDetector()
        emotion_analysis = await emotion_detector.detect_emotions(text_content.strip())
        
        # Create final sentiment based on best model's sentiment
        best_model_result = None
        if best_performing_model:
            for result in model_results:
                if result.model_name == best_performing_model:
                    best_model_result = result
                    break
        
        if best_model_result and best_model_result.sentiment:
            final_sentiment = best_model_result.sentiment
        else:
            final_sentiment = SentimentResponse(
                positive=0.3,
                negative=0.2,
                neutral=0.5,
                overall="neutral",
                confidence=0.75
            )
        
        final_emotions = EmotionResponse(
            joy=emotion_analysis['joy'],
            sadness=emotion_analysis['sadness'],
            anger=emotion_analysis['anger'],
            fear=emotion_analysis['fear'],
            surprise=emotion_analysis['surprise'],
            disgust=emotion_analysis['disgust'],
            dominant_emotion=emotion_analysis['dominant_emotion'],
            confidence=emotion_analysis['confidence']
        )
        
        # Create final risk assessment with intelligent recommendations
        # Generate intelligent recommendations based on analysis results
        analysis_data = {
            'risk_level': best_model_result.risk.level if best_model_result and best_model_result.risk else 'low',
            'emotions': emotion_analysis,
            'sentiment': final_sentiment.dict() if final_sentiment else {'overall': 'neutral'},
            'text_content': text_content
        }
        
        intelligent_recommendations = generate_analysis_recommendations(analysis_data)
        
        # Use best model's risk data but with intelligent recommendations
        if best_model_result and best_model_result.risk:
            final_risk = RiskResponse(
                level=best_model_result.risk.level,
                score=best_model_result.risk.score,
                factors=best_model_result.risk.factors,
                recommendations=intelligent_recommendations,
                confidence=best_model_result.risk.confidence
            )
        else:
            final_risk = RiskResponse(
                level="low",
                score=0.2,
                factors=["Overall analysis indicates low risk"],
                recommendations=intelligent_recommendations,
                confidence=0.75
            )
        
        # Try to store analysis result (skip if database not available)
        try:
            analysis_collection = await get_analysis_collection()
            # Store simplified analysis result
            analysis_data = {
                "analysis_id": analysis_id,
                "analysis_type": "csv",
                "file_id": csv_data.file_id,
                "status": "completed",
                "successful_models": successful_models,
                "failed_models": failed_models,
                "created_at": datetime.utcnow()
            }
            await analysis_collection.insert_one(analysis_data)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not store analysis result: {str(e)}")
        
        return AnalysisResponse(
            analysis_id=analysis_id,
            analysis_type="csv",
            file_id=csv_data.file_id,
            model_results=model_results,
            final_sentiment=final_sentiment,
            final_emotions=final_emotions,
            final_risk=final_risk,
            best_model=best_performing_model,
            successful_models=successful_models,
            failed_models=failed_models,
            processing_time=len(model_results) * 0.5,  # Estimate
            status="completed",
            created_at=datetime.utcnow(),
            confidence=_calculate_realistic_confidence(model_results),
            message="CSV analysis completed successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in CSV analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.post("/analyze-text", response_model=AnalysisResponse)
async def analyze_simple_text(request: SimpleTextAnalysisRequest):
    """
    Analyze simple text input directly without CSV formatting
    
    Args:
        request: SimpleTextAnalysisRequest containing text to analyze
        
    Returns:
        AnalysisResponse: Analysis results from all models
    """
    try:
        logger.info(f"üîÆ Simple text analysis request for: {request.text[:50]}...")
        
        if not request.text or request.text.strip() == "":
            raise HTTPException(
                status_code=400,
                detail="No text provided for analysis"
            )
        
        # Get model manager from app state
        from app.main import app
        if not hasattr(app.state, 'model_manager'):
            raise HTTPException(
                status_code=503,
                detail="AI models not available"
            )
        
        model_manager = app.state.model_manager
        
        # Run AI analysis on the simple text
        model_predictions = await model_manager.predict_all_models(request.text.strip())
        
        if not model_predictions:
            raise HTTPException(
                status_code=500,
                detail="No model predictions were successful"
            )
        
        # Calculate aggregated results (use best performing model)
        best_model = max(model_predictions, key=lambda x: x.accuracy if x.status == "success" else 0)
        
        # Create analysis result
        analysis_id = str(uuid.uuid4())
        
        analysis_result = AnalysisResult(
            analysis_id=analysis_id,
            analysis_type="text",
            model_predictions=model_predictions,
            final_sentiment=best_model.sentiment if best_model.sentiment else None,
            final_emotions=best_model.emotions if best_model.emotions else None,
            final_risk=best_model.risk if best_model.risk else None,
            best_performing_model=best_model.model_name,
            successful_models=[p.model_name for p in model_predictions if p.status == "success"],
            failed_models=[p.model_name for p in model_predictions if p.status == "failed"],
            processing_time=sum(p.prediction_time for p in model_predictions),
            status="completed"
        )
        
        # Store analysis result (optional - skip if database not available)
        try:
            analysis_collection = await get_analysis_collection()
            await analysis_collection.insert_one(analysis_result.dict(exclude={"id"}))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not store analysis result: {str(e)}")
        
        # Convert model predictions to response format
        from app.models.request_models import ModelResultResponse, SentimentResponse, EmotionResponse, RiskResponse
        
        model_results = []
        for pred in model_predictions:
            sentiment_resp = None
            if pred.sentiment:
                sentiment_resp = SentimentResponse(
                    positive=pred.sentiment.positive,
                    negative=pred.sentiment.negative,
                    neutral=pred.sentiment.neutral,
                    overall=pred.sentiment.overall,
                    confidence=pred.sentiment.confidence
                )
            
            emotion_resp = None
            if pred.emotions:
                emotion_resp = EmotionResponse(
                    joy=pred.emotions.joy,
                    sadness=pred.emotions.sadness,
                    anger=pred.emotions.anger,
                    fear=pred.emotions.fear,
                    surprise=pred.emotions.surprise,
                    disgust=pred.emotions.disgust,
                    dominant_emotion=pred.emotions.dominant_emotion,
                    confidence=pred.emotions.confidence
                )
            
            risk_resp = None
            if pred.risk:
                risk_resp = RiskResponse(
                    level=pred.risk.level,
                    score=pred.risk.score,
                    factors=pred.risk.factors,
                    recommendations=pred.risk.recommendations,
                    confidence=pred.risk.confidence
                )
            
            model_result = ModelResultResponse(
                model_name=pred.model_name,
                accuracy=pred.accuracy,
                processing_time=pred.prediction_time,
                status=pred.status,
                confidence=pred.confidence,
                sentiment=sentiment_resp,
                emotions=emotion_resp,
                risk=risk_resp,
                error_message=pred.error_message
            )
            model_results.append(model_result)
        
        # Create final aggregated results with proper type conversion
        final_sentiment = None
        if best_model.sentiment:
            final_sentiment = SentimentResponse(
                positive=best_model.sentiment.positive,
                negative=best_model.sentiment.negative,
                neutral=best_model.sentiment.neutral,
                overall=best_model.sentiment.overall,
                confidence=best_model.sentiment.confidence
            )
        
        final_emotions = None
        if best_model.emotions:
            final_emotions = EmotionResponse(
                joy=best_model.emotions.joy,
                sadness=best_model.emotions.sadness,
                anger=best_model.emotions.anger,
                fear=best_model.emotions.fear,
                surprise=best_model.emotions.surprise,
                disgust=best_model.emotions.disgust,
                dominant_emotion=best_model.emotions.dominant_emotion,
                confidence=best_model.emotions.confidence
            )
        
        final_risk = None
        if best_model.risk:
            final_risk = RiskResponse(
                level=best_model.risk.level,
                score=best_model.risk.score,
                factors=best_model.risk.factors,
                recommendations=best_model.risk.recommendations,
                confidence=best_model.risk.confidence
            )
        
        return AnalysisResponse(
            analysis_id=analysis_id,
            analysis_type="text",
            model_results=model_results,
            final_sentiment=final_sentiment,
            final_emotions=final_emotions,
            final_risk=final_risk,
            best_model=best_model.model_name,
            successful_models=[p.model_name for p in model_predictions if p.status == "success"],
            failed_models=[p.model_name for p in model_predictions if p.status == "failed"],
            processing_time=sum(p.prediction_time for p in model_predictions),
            status="completed",
            created_at=datetime.utcnow(),
            confidence=_calculate_realistic_confidence(model_results)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in simple text analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
